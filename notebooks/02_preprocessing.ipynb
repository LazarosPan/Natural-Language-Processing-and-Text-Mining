{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e7334c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T11:47:52.501970Z",
     "iopub.status.busy": "2025-06-01T11:47:52.501902Z",
     "iopub.status.idle": "2025-06-01T11:48:23.367665Z",
     "shell.execute_reply": "2025-06-01T11:48:23.367407Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique questions across all splits: 537,359\n",
      "SBERT embedding matrix shape: (537359, 768)\n",
      "\n",
      "Pre-processing complete. Files now in data/processed/:\n",
      " * clean_questions.npy\n",
      " * question_embeddings.npy\n",
      " * question_meta.csv\n",
      " * st_1253c25d.npy\n"
     ]
    }
   ],
   "source": [
    "# ────────────────────────────────────────────────────────────────\n",
    "# notebooks/02_preprocessing.ipynb\n",
    "# ----------------------------------------------------------------\n",
    "# 0) Enable src/ on PYTHONPATH\n",
    "%run setup.py\n",
    "\n",
    "# 1) Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, glob\n",
    "from pathlib import Path\n",
    "from src.preprocessing import clean_and_stats, build_st_embeddings\n",
    "\n",
    "# 2) Read split CSVs\n",
    "SPLIT_DIR = Path(\"../data/splits\")\n",
    "train_df  = pd.read_csv(SPLIT_DIR / \"train.csv\").dropna(subset=[\"question1\", \"question2\"])\n",
    "valid_df  = pd.read_csv(SPLIT_DIR / \"valid.csv\").dropna(subset=[\"question1\", \"question2\"])\n",
    "test_df   = pd.read_csv(SPLIT_DIR / \"test.csv\").dropna(subset=[\"question1\", \"question2\"])\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# IMPORTANT  ❗\n",
    "# We UNION all questions so every qid appearing in *any* split\n",
    "# has  (1) a cleaned string,  (2) an SBERT embedding row.\n",
    "# This is NOT data-leakage because the encoder is PRETRAINED + FROZEN.\n",
    "# -----------------------------------------------------------------\n",
    "all_questions = (\n",
    "    pd.concat(\n",
    "        [\n",
    "            train_df[[\"question1\", \"question2\"]],\n",
    "            valid_df[[\"question1\", \"question2\"]],\n",
    "            test_df[[\"question1\", \"question2\"]],\n",
    "        ],\n",
    "        axis=0,\n",
    "    )\n",
    "    .stack()\n",
    "    .unique()\n",
    ")\n",
    "\n",
    "print(f\"Unique questions across all splits: {len(all_questions):,}\")\n",
    "\n",
    "# 3) Clean texts & stats\n",
    "cleaned, char_len, word_cnt = [], [], []\n",
    "for q in all_questions:\n",
    "    c, ln, wc = clean_and_stats(q)\n",
    "    cleaned.append(c)\n",
    "    char_len.append(ln)\n",
    "    word_cnt.append(wc)\n",
    "\n",
    "# 4) Persist per-question artefacts\n",
    "OUT_DIR = Path(\"../data/processed\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        \"question\": all_questions,\n",
    "        \"clean\": cleaned,\n",
    "        \"len\": char_len,\n",
    "        \"words\": word_cnt,\n",
    "    }\n",
    ").to_csv(OUT_DIR / \"question_meta.csv\", index=False)\n",
    "\n",
    "np.save(OUT_DIR / \"clean_questions.npy\", np.array(cleaned, dtype=object))\n",
    "\n",
    "# 5) SBERT embeddings (Quora-DistilBERT, 768-d)\n",
    "#    – Delete any stale file first to avoid 384-d confusion\n",
    "emb_fp = OUT_DIR / \"question_embeddings.npy\"\n",
    "if emb_fp.exists():\n",
    "    emb_fp.unlink()\n",
    "    print(\"Deleted stale question_embeddings.npy\")\n",
    "\n",
    "emb = build_st_embeddings(\n",
    "    corpus     = cleaned,\n",
    "    model_name = \"sentence-transformers/distilbert-base-nli-stsb-quora-ranking\",\n",
    "    cache_dir  = OUT_DIR,          # hashed cache lives here\n",
    "    batch_size = 512,\n",
    "    out_fp     = emb_fp            # canonical file for downstream code\n",
    ")\n",
    "\n",
    "print(\"SBERT embedding matrix shape:\", emb.shape)  # (N_q, 768)\n",
    "\n",
    "print(\"\\nPre-processing complete. Files now in data/processed/:\")\n",
    "for p in sorted(glob.glob(str(OUT_DIR / '*'))):\n",
    "    print(\" *\", os.path.basename(p))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
