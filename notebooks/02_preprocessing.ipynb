{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e7334c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique questions across all splits: 537359\n",
      "Preprocessing complete. Written to: ../data/processed\n",
      "[PosixPath('../data/processed/clean_questions.npy'), PosixPath('../data/processed/X_train.npy'), PosixPath('../data/processed/question_meta.csv'), PosixPath('../data/processed/train_cross_scores.npy'), PosixPath('../data/processed/question_embeddings.npy')]\n"
     ]
    }
   ],
   "source": [
    "# ────────────────────────────────────────────────────────────────\n",
    "# notebooks/02_preprocessing.ipynb\n",
    "# ===============================================\n",
    "\n",
    "# 0) Enable our src/ folder on the PYTHONPATH\n",
    "%run setup.py\n",
    "\n",
    "# 1) Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "from src.preprocessing import clean_and_stats, build_st_embeddings\n",
    "\n",
    "# 2) Paths to the **three** pre-split CSVs\n",
    "SPLIT_DIR = Path(\"../data/splits\")\n",
    "train_df  = pd.read_csv(SPLIT_DIR / \"train.csv\").dropna(subset=[\"question1\",\"question2\"])\n",
    "valid_df  = pd.read_csv(SPLIT_DIR / \"valid.csv\").dropna(subset=[\"question1\",\"question2\"])\n",
    "test_df   = pd.read_csv(SPLIT_DIR / \"test.csv\").dropna(subset=[\"question1\",\"question2\"])\n",
    "\n",
    "# 3) Collect **every unique** raw question string from train ∪ valid ∪ test\n",
    "all_questions = pd.concat(\n",
    "    [\n",
    "        train_df[[\"question1\", \"question2\"]],\n",
    "        valid_df[[\"question1\", \"question2\"]],\n",
    "        test_df[[\"question1\", \"question2\"]],\n",
    "    ],\n",
    "    axis=0\n",
    ").stack().unique()\n",
    "\n",
    "print(f\"Unique questions across all splits: {len(all_questions)}\")\n",
    "\n",
    "# 4) Clean each unique question and collect stats (char length, word count)\n",
    "cleaned, char_len, word_cnt = [], [], []\n",
    "for q in all_questions:\n",
    "    c, ln, wc = clean_and_stats(q)\n",
    "    cleaned.append(c)\n",
    "    char_len.append(ln)\n",
    "    word_cnt.append(wc)\n",
    "\n",
    "# 5) Save per-question artifacts to disk\n",
    "OUT_DIR = Path(\"../data/processed\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 5a) question_meta.csv → columns: [\"question\", \"clean\", \"len\", \"words\"]\n",
    "pd.DataFrame({\n",
    "    \"question\": all_questions,\n",
    "    \"clean\":    cleaned,\n",
    "    \"len\":      char_len,\n",
    "    \"words\":    word_cnt\n",
    "}).to_csv(OUT_DIR / \"question_meta.csv\", index=False)\n",
    "\n",
    "# 5b) clean_questions.npy → array of cleaned strings (dtype=object)\n",
    "np.save(OUT_DIR / \"clean_questions.npy\", np.array(cleaned, dtype=object))\n",
    "\n",
    "# 6) Pre-compute MiniLM-L6 embeddings for every cleaned question (bi-encoder caching)\n",
    "#    On first run, this will download the model. Afterwards, it mmap-loads for speed.\n",
    "emb = build_st_embeddings(\n",
    "    corpus      = cleaned,\n",
    "    model_name  = \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    cache_dir   = \"models\",\n",
    "    batch_size  = 512\n",
    ")\n",
    "np.save(OUT_DIR / \"question_embeddings.npy\", emb)\n",
    "\n",
    "print(\"Preprocessing complete. Written to:\", OUT_DIR)\n",
    "print(list(OUT_DIR.iterdir()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
