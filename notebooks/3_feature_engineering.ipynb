{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe580f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T20:23:35.650518Z",
     "iopub.status.busy": "2025-06-02T20:23:35.650448Z",
     "iopub.status.idle": "2025-06-02T20:24:30.519458Z",
     "shell.execute_reply": "2025-06-02T20:24:30.519252Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of TRAIN pairs: 323,613\n",
      "Number of VALID pairs: 40,710\n",
      "Number of TEST  pairs: 39,964\n"
     ]
    }
   ],
   "source": [
    "# ────────────────────────────────────────────────────────────────\n",
    "# notebooks/3_feature_engineering.ipynb ─ Cell 1\n",
    "# ===============================================\n",
    "\n",
    "# 0) Ensure that src/ is on PYTHONPATH\n",
    "%run setup.py\n",
    "\n",
    "# 1) Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# 2) Paths to per‐question artifacts & splits\n",
    "PROCESSED_DIR    = Path(\"../data/processed\")\n",
    "meta_fp          = PROCESSED_DIR / \"question_meta.csv\"\n",
    "clean_fp         = PROCESSED_DIR / \"clean_questions.npy\"\n",
    "split_dir        = Path(\"../data/splits\")\n",
    "\n",
    "# 3) Load question_meta & cleaned questions\n",
    "if not meta_fp.exists():\n",
    "    raise FileNotFoundError(f\"Missing '{meta_fp}'. Run preprocessing to generate question_meta.csv.\")\n",
    "meta_df = pd.read_csv(meta_fp)\n",
    "\n",
    "if not clean_fp.exists():\n",
    "    raise FileNotFoundError(f\"Missing '{clean_fp}'. Run preprocessing to generate clean_questions.npy.\")\n",
    "clean_questions = np.load(clean_fp, allow_pickle=True).tolist()\n",
    "\n",
    "if len(clean_questions) != len(meta_df):\n",
    "    raise RuntimeError(\n",
    "        f\"Length mismatch: clean_questions.npy has {len(clean_questions)} entries, \"\n",
    "        f\"but question_meta.csv has {len(meta_df)} rows.\"\n",
    "    )\n",
    "\n",
    "# 4) Build a lookup from question text -> its integer ID (row index in question_meta.csv)\n",
    "qid_lookup = {q: idx for idx, q in enumerate(meta_df[\"question\"].astype(str).tolist())}\n",
    "\n",
    "# 5) Load & map TRAIN/VALID/TEST pairs -> add qid1/qid2\n",
    "def _load_and_map_split(split_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads \"../data/splits/{split_name}.csv\", drops any NA pairs,\n",
    "    and adds two columns: qid1 and qid2. Raises if any question strings fail mapping.\n",
    "    \"\"\"\n",
    "    fp = split_dir / f\"{split_name}.csv\"\n",
    "    if not fp.exists():\n",
    "        raise FileNotFoundError(f\"Missing split file: {fp}\")\n",
    "    df = (\n",
    "        pd.read_csv(fp)\n",
    "          .dropna(subset=[\"question1\", \"question2\"])\n",
    "          .assign(\n",
    "              qid1=lambda d: d.question1.map(qid_lookup).astype(int),\n",
    "              qid2=lambda d: d.question2.map(qid_lookup).astype(int)\n",
    "          )\n",
    "    )\n",
    "    if df.qid1.isna().any() or df.qid2.isna().any():\n",
    "        raise ValueError(f\"Some questions in {split_name.upper()} cannot be mapped to question_meta.csv.\")\n",
    "    return df\n",
    "\n",
    "train_df = _load_and_map_split(\"train\")\n",
    "valid_df = _load_and_map_split(\"valid\")\n",
    "test_df  = _load_and_map_split(\"test\")\n",
    "\n",
    "print(f\"Number of TRAIN pairs: {len(train_df):,}\")\n",
    "print(f\"Number of VALID pairs: {len(valid_df):,}\")\n",
    "print(f\"Number of TEST  pairs: {len(test_df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91171b79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T20:24:30.520419Z",
     "iopub.status.busy": "2025-06-02T20:24:30.520315Z",
     "iopub.status.idle": "2025-06-02T20:43:19.329132Z",
     "shell.execute_reply": "2025-06-02T20:43:19.328695Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1749000421.989491   32316 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1749000421.992036   32316 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1749000421.998423   32316 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749000421.998434   32316 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749000421.998435   32316 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749000421.998436   32316 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[31mAttributeError\u001b[39m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[31mAttributeError\u001b[39m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[31mAttributeError\u001b[39m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[31mAttributeError\u001b[39m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[31mAttributeError\u001b[39m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TRACK dim=384 ===\n",
      "1) [SKIP] X_train_384_ipca.npy already exists (shape=(323613, 3))\n",
      "2) [SKIP] X_valid_384_ipca.npy already exists (shape=(40710, 3))\n",
      "3) [SKIP] X_test_384_ipca.npy already exists (shape=(39964, 3))\n",
      "4) [SKIP] X_train_384_umap10.npy already exists (shape=(323613, 10))\n",
      "5) [SKIP] X_valid_384_umap10.npy already exists (shape=(40710, 10))\n",
      "6) [SKIP] X_test_384_umap10.npy already exists (shape=(39964, 10))\n",
      "=== Completed IPCA & UMAP for dim=384 ===\n",
      "\n",
      "\n",
      "=== TRACK dim=768 ===\n",
      "1) Fitting IPCA-only features on TRAIN…\n",
      "   -> Saved X_train_768_ipca.npy (shape=(323613, 3))\n",
      "2) Applying saved IPCA on VALID…\n",
      "   -> Saved X_valid_768_ipca.npy (shape=(40710, 3))\n",
      "3) Applying saved IPCA on TEST…\n",
      "   -> Saved X_test_768_ipca.npy (shape=(39964, 3))\n",
      "4) Fitting UMAP-only features on TRAIN…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lazar/.venvs/data-science/lib/python3.13/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Saved X_train_768_umap10.npy (shape=(323613, 10))\n",
      "5) Applying saved UMAP on VALID…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lazar/.venvs/data-science/lib/python3.13/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Saved X_valid_768_umap10.npy (shape=(40710, 10))\n",
      "6) Applying saved UMAP on TEST…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lazar/.venvs/data-science/lib/python3.13/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Saved X_test_768_umap10.npy (shape=(39964, 10))\n",
      "=== Completed IPCA & UMAP for dim=768 ===\n",
      "\n",
      "All feature-engineering tracks (IPCA / UMAP) complete.\n"
     ]
    }
   ],
   "source": [
    "# ────────────────────────────────────────────────────────────────\n",
    "# notebooks/03_feature_engineering.ipynb ─ Cell 2 (updated)\n",
    "# ===============================================\n",
    "\n",
    "# 0) Ensure that src/ is on PYTHONPATH (again, if you restarted kernel)\n",
    "%run setup.py\n",
    "\n",
    "# 1) Import build_features\n",
    "from src.features import build_features\n",
    "\n",
    "# 2) Common settings\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "PROCESSED_DIR = Path(\"../data/processed\")\n",
    "DIM_LIST      = [384, 768]\n",
    "\n",
    "# 3) UMAP target dimension (adjust as desired)\n",
    "n_components_umap = 10\n",
    "\n",
    "# 4) Loop over each SBERT track (384 & 768)\n",
    "for dim in DIM_LIST:\n",
    "    model_dir = Path(f\"../models/features_{dim}\")\n",
    "    model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    emb_fp = PROCESSED_DIR / f\"question_embeddings_{dim}.npy\"\n",
    "    if not emb_fp.exists():\n",
    "        raise FileNotFoundError(f\"Missing embeddings for dim={dim}: {emb_fp}\")\n",
    "\n",
    "    print(f\"\\n=== TRACK dim={dim} ===\")\n",
    "\n",
    "    #\n",
    "    # 1) IPCA-only (95%) – TRAIN\n",
    "    #\n",
    "    out_tr_ipca = PROCESSED_DIR / f\"X_train_{dim}_ipca.npy\"\n",
    "    if out_tr_ipca.exists():\n",
    "        X_tr_ipca = np.load(out_tr_ipca, mmap_mode=\"r\")\n",
    "        print(f\"1) [SKIP] X_train_{dim}_ipca.npy already exists (shape={X_tr_ipca.shape})\")\n",
    "    else:\n",
    "        print(\"1) Fitting IPCA-only features on TRAIN…\")\n",
    "        X_tr_ipca = build_features(\n",
    "            pair_df         = train_df,\n",
    "            clean_questions = clean_questions,\n",
    "            meta_df         = meta_df,\n",
    "            embedding_path  = str(emb_fp),\n",
    "            cache_dir       = str(model_dir),\n",
    "            cross_cache     = str(PROCESSED_DIR / f\"train_cross_{dim}.npy\"),\n",
    "            fit_pca         = True,    # chunked two-pass IPCA\n",
    "            features_cache  = str(PROCESSED_DIR / f\"train_raw_{dim}.npy\"),\n",
    "            reduction       = \"ipca\",\n",
    "            n_components    = None,    # auto k95\n",
    "        )\n",
    "        np.save(out_tr_ipca, X_tr_ipca)\n",
    "        print(f\"   -> Saved X_train_{dim}_ipca.npy (shape={X_tr_ipca.shape})\")\n",
    "\n",
    "    #\n",
    "    # 2) IPCA-only (95%) – VALID\n",
    "    #\n",
    "    out_val_ipca = PROCESSED_DIR / f\"X_valid_{dim}_ipca.npy\"\n",
    "    if out_val_ipca.exists():\n",
    "        X_val_ipca = np.load(out_val_ipca, mmap_mode=\"r\")\n",
    "        print(f\"2) [SKIP] X_valid_{dim}_ipca.npy already exists (shape={X_val_ipca.shape})\")\n",
    "    else:\n",
    "        print(\"2) Applying saved IPCA on VALID…\")\n",
    "        X_val_ipca = build_features(\n",
    "            pair_df         = valid_df,\n",
    "            clean_questions = clean_questions,\n",
    "            meta_df         = meta_df,\n",
    "            embedding_path  = str(emb_fp),\n",
    "            cache_dir       = str(model_dir),\n",
    "            cross_cache     = str(PROCESSED_DIR / f\"valid_cross_{dim}.npy\"),\n",
    "            fit_pca         = False,  # reuse existing IPCA pickles\n",
    "            features_cache  = str(PROCESSED_DIR / f\"valid_raw_{dim}.npy\"),\n",
    "            reduction       = \"ipca\",\n",
    "            n_components    = None,\n",
    "        )\n",
    "        np.save(out_val_ipca, X_val_ipca)\n",
    "        print(f\"   -> Saved X_valid_{dim}_ipca.npy (shape={X_val_ipca.shape})\")\n",
    "\n",
    "    #\n",
    "    # 3) IPCA-only (95%) – TEST\n",
    "    #\n",
    "    out_te_ipca = PROCESSED_DIR / f\"X_test_{dim}_ipca.npy\"\n",
    "    if out_te_ipca.exists():\n",
    "        X_te_ipca = np.load(out_te_ipca, mmap_mode=\"r\")\n",
    "        print(f\"3) [SKIP] X_test_{dim}_ipca.npy already exists (shape={X_te_ipca.shape})\")\n",
    "    else:\n",
    "        print(\"3) Applying saved IPCA on TEST…\")\n",
    "        X_te_ipca = build_features(\n",
    "            pair_df         = test_df,\n",
    "            clean_questions = clean_questions,\n",
    "            meta_df         = meta_df,\n",
    "            embedding_path  = str(emb_fp),\n",
    "            cache_dir       = str(model_dir),\n",
    "            cross_cache     = str(PROCESSED_DIR / f\"test_cross_{dim}.npy\"),\n",
    "            fit_pca         = False,\n",
    "            features_cache  = str(PROCESSED_DIR / f\"test_raw_{dim}.npy\"),\n",
    "            reduction       = \"ipca\",\n",
    "            n_components    = None,\n",
    "        )\n",
    "        np.save(out_te_ipca, X_te_ipca)\n",
    "        print(f\"   -> Saved X_test_{dim}_ipca.npy (shape={X_te_ipca.shape})\")\n",
    "\n",
    "\n",
    "    # ────────────────────────────────────────────────────────────────────────────\n",
    "    # 4) UMAP-only – TRAIN\n",
    "    #\n",
    "    out_tr_umap = PROCESSED_DIR / f\"X_train_{dim}_umap{n_components_umap}.npy\"\n",
    "    if out_tr_umap.exists():\n",
    "        X_tr_umap = np.load(out_tr_umap, mmap_mode=\"r\")\n",
    "        print(f\"4) [SKIP] X_train_{dim}_umap{n_components_umap}.npy already exists (shape={X_tr_umap.shape})\")\n",
    "    else:\n",
    "        print(\"4) Fitting UMAP-only features on TRAIN…\")\n",
    "        X_tr_umap = build_features(\n",
    "            pair_df         = train_df,\n",
    "            clean_questions = clean_questions,\n",
    "            meta_df         = meta_df,\n",
    "            embedding_path  = str(emb_fp),\n",
    "            cache_dir       = str(model_dir),\n",
    "            cross_cache     = str(PROCESSED_DIR / f\"train_cross_{dim}.npy\"),\n",
    "            fit_pca         = True,   # ensures raw features are built, then chunked IPCA->k95\n",
    "            features_cache  = str(PROCESSED_DIR / f\"train_raw_{dim}.npy\"),\n",
    "            reduction       = \"umap\",\n",
    "            n_components    = n_components_umap,\n",
    "        )\n",
    "        np.save(out_tr_umap, X_tr_umap)\n",
    "        print(f\"   -> Saved X_train_{dim}_umap{n_components_umap}.npy (shape={X_tr_umap.shape})\")\n",
    "\n",
    "    #\n",
    "    # 5) UMAP-only – VALID\n",
    "    #\n",
    "    out_val_umap = PROCESSED_DIR / f\"X_valid_{dim}_umap{n_components_umap}.npy\"\n",
    "    if out_val_umap.exists():\n",
    "        X_val_umap = np.load(out_val_umap, mmap_mode=\"r\")\n",
    "        print(f\"5) [SKIP] X_valid_{dim}_umap{n_components_umap}.npy already exists (shape={X_val_umap.shape})\")\n",
    "    else:\n",
    "        print(\"5) Applying saved UMAP on VALID…\")\n",
    "        X_val_umap = build_features(\n",
    "            pair_df         = valid_df,\n",
    "            clean_questions = clean_questions,\n",
    "            meta_df         = meta_df,\n",
    "            embedding_path  = str(emb_fp),\n",
    "            cache_dir       = str(model_dir),\n",
    "            cross_cache     = str(PROCESSED_DIR / f\"valid_cross_{dim}.npy\"),\n",
    "            fit_pca         = False,  # reuse existing UMAP pickles\n",
    "            features_cache  = str(PROCESSED_DIR / f\"valid_raw_{dim}.npy\"),\n",
    "            reduction       = \"umap\",\n",
    "            n_components    = n_components_umap,\n",
    "        )\n",
    "        np.save(out_val_umap, X_val_umap)\n",
    "        print(f\"   -> Saved X_valid_{dim}_umap{n_components_umap}.npy (shape={X_val_umap.shape})\")\n",
    "\n",
    "    #\n",
    "    # 6) UMAP-only – TEST\n",
    "    #\n",
    "    out_te_umap = PROCESSED_DIR / f\"X_test_{dim}_umap{n_components_umap}.npy\"\n",
    "    if out_te_umap.exists():\n",
    "        X_te_umap = np.load(out_te_umap, mmap_mode=\"r\")\n",
    "        print(f\"6) [SKIP] X_test_{dim}_umap{n_components_umap}.npy already exists (shape={X_te_umap.shape})\")\n",
    "    else:\n",
    "        print(\"6) Applying saved UMAP on TEST…\")\n",
    "        X_te_umap = build_features(\n",
    "            pair_df         = test_df,\n",
    "            clean_questions = clean_questions,\n",
    "            meta_df         = meta_df,\n",
    "            embedding_path  = str(emb_fp),\n",
    "            cache_dir       = str(model_dir),\n",
    "            cross_cache     = str(PROCESSED_DIR / f\"test_cross_{dim}.npy\"),\n",
    "            fit_pca         = False,\n",
    "            features_cache  = str(PROCESSED_DIR / f\"test_raw_{dim}.npy\"),\n",
    "            reduction       = \"umap\",\n",
    "            n_components    = n_components_umap,\n",
    "        )\n",
    "        np.save(out_te_umap, X_te_umap)\n",
    "        print(f\"   -> Saved X_test_{dim}_umap{n_components_umap}.npy (shape={X_te_umap.shape})\")\n",
    "\n",
    "    print(f\"=== Completed IPCA & UMAP for dim={dim} ===\\n\")\n",
    "\n",
    "print(\"All feature-engineering tracks (IPCA / UMAP) complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
