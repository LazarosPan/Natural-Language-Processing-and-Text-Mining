{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfe580f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training pairs: 323,554\n",
      "-> 442,917 unique questions in TRAIN split\n",
      "Fitting TF-IDF (word ngrams) on TRAIN... done!\n",
      "Fitting TF-IDF (char ngrams) on TRAIN... done!\n",
      "Saved TF-IDF vectorisers -> models/tfidf_w.pkl  &  models/tfidf_c.pkl\n",
      "Saved SVD models -> models/svd_w_150.pkl  &  models/svd_c_100.pkl\n",
      "TF-IDF & SVD have been fitted on TRAIN ONLY.\n"
     ]
    }
   ],
   "source": [
    "# ────────────────────────────────────────────────────────────────\n",
    "# notebooks/03_feature_engineering.ipynb ─ Cell 1\n",
    "# ===============================================\n",
    "\n",
    "# 0) Ensure that src/ is on PYTHONPATH\n",
    "%run setup.py\n",
    "\n",
    "# 1) Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# 2) Paths to per-question artefacts & splits\n",
    "PROCESSED_DIR    = Path(\"../data/processed\")\n",
    "meta_df          = pd.read_csv(PROCESSED_DIR / \"question_meta.csv\")\n",
    "clean_questions  = np.load(PROCESSED_DIR / \"clean_questions.npy\", allow_pickle=True).tolist()\n",
    "qid_lookup       = {q: idx for idx, q in enumerate(meta_df[\"question\"])}\n",
    "\n",
    "# 3) Load & map TRAIN pairs -> qid1, qid2\n",
    "train_df = (\n",
    "    pd.read_csv(\"../data/splits/train.csv\")\n",
    "    .dropna(subset=[\"question1\",\"question2\"])\n",
    "    .assign(\n",
    "        qid1 = lambda d: d.question1.map(qid_lookup).astype(int),\n",
    "        qid2 = lambda d: d.question2.map(qid_lookup).astype(int)\n",
    "    )\n",
    ")\n",
    "\n",
    "# Sanity check: no NaNs\n",
    "assert not train_df[\"qid1\"].isna().any() and not train_df[\"qid2\"].isna().any()\n",
    "\n",
    "print(f\"Number of training pairs: {len(train_df):,}\")\n",
    "train_qids = np.unique(np.concatenate([train_df.qid1.values, train_df.qid2.values]))\n",
    "print(f\"-> {len(train_qids):,} unique questions in TRAIN split\")\n",
    "\n",
    "# 4) Subset of cleaned questions that actually appear in train\n",
    "train_corpus = [clean_questions[i] for i in train_qids]  # List[str]\n",
    "\n",
    "# 5) Fit TF-IDF (word & char) ON TRAIN ONLY\n",
    "vec_w = TfidfVectorizer(ngram_range=(1,2), min_df=3, sublinear_tf=True)\n",
    "vec_c = TfidfVectorizer(analyzer=\"char\", ngram_range=(3,5), min_df=10)\n",
    "\n",
    "print(\"Fitting TF-IDF (word ngrams) on TRAIN...\", end=\" \")\n",
    "vec_w.fit(train_corpus)\n",
    "print(\"done!\")\n",
    "print(\"Fitting TF-IDF (char ngrams) on TRAIN...\", end=\" \")\n",
    "vec_c.fit(train_corpus)\n",
    "print(\"done!\")\n",
    "\n",
    "# 6) Persist both vectorisers\n",
    "MODEL_DIR = Path(\"../models/features/\")\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "joblib.dump(vec_w, MODEL_DIR / \"tfidf_w.pkl\")\n",
    "joblib.dump(vec_c, MODEL_DIR / \"tfidf_c.pkl\")\n",
    "print(\"Saved TF-IDF vectorisers -> models/tfidf_w.pkl  &  models/tfidf_c.pkl\")\n",
    "\n",
    "# 7) Fit SVD ON TRAIN TF-IDF matrices (word & char)\n",
    "Z_w = vec_w.transform(train_corpus)  # sparse (n_train_q, V_w)\n",
    "Z_c = vec_c.transform(train_corpus)  # sparse (n_train_q, V_c)\n",
    "\n",
    "svd_w = TruncatedSVD(n_components=150, random_state=42).fit(Z_w)\n",
    "svd_c = TruncatedSVD(n_components=100, random_state=42).fit(Z_c)\n",
    "\n",
    "# 8) Persist the **SVD models** themselves (not just projections)\n",
    "joblib.dump(svd_w, MODEL_DIR / \"svd_w_150.pkl\")\n",
    "joblib.dump(svd_c, MODEL_DIR / \"svd_c_100.pkl\")\n",
    "print(\"Saved SVD models -> models/svd_w_150.pkl  &  models/svd_c_100.pkl\")\n",
    "\n",
    "print(\"TF-IDF & SVD have been fitted on TRAIN ONLY.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91171b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Building TRAIN features + fitting IncrementalPCA (95% variance)…\n",
      "  [features.py]  1st pass of IncrementalPCA(n_components=None) to compute variance…\n",
      "  [features.py]  #components → 3 to retain 95% variance\n",
      "  [features.py]  2nd pass of IncrementalPCA(n_components=k95) to fit_transform…\n",
      "* Saved X_train.npy with shape (323554, 3) (d_reduced ≪ 3598)\n",
      "- Building VALID features + applying saved PCA…\n",
      "* Saved X_valid.npy with shape (40087, 3)\n",
      "* Building TEST features + applying saved PCA…\n",
      "* Saved X_test.npy with shape (40646, 3)\n",
      "\n",
      "Feature‐engineering + IncrementalPCA complete. Files now in data/processed:\n",
      "  * X_train.npy (TRAIN reduced → 95%)\n",
      "  * X_valid.npy (VALID reduced)\n",
      "  * X_test.npy  (TEST reduced)\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# notebooks/03_feature_engineering.ipynb ― Cell 2\n",
    "# ===============================================\n",
    "\n",
    "# 0) Ensure that src/ is on PYTHONPATH\n",
    "%run setup.py\n",
    "\n",
    "# 1) Import\n",
    "from src.features import build_features\n",
    "\n",
    "# 2) Paths\n",
    "PROCESSED_DIR = Path(\"../data/processed\")\n",
    "MODEL_DIR     = Path(\"../models/features\")     # where TF-IDF/SVD/PCA live\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 3) Load per-question artefacts\n",
    "meta_df         = pd.read_csv(PROCESSED_DIR / \"question_meta.csv\")\n",
    "clean_questions = np.load(PROCESSED_DIR / \"clean_questions.npy\", allow_pickle=True).tolist()\n",
    "qid_lookup      = {q: idx for idx, q in enumerate(meta_df[\"question\"])}\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 4) BUILD & SAVE PCA-REDUCED FEATURES FOR TRAIN SPLIT\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "train_df = (\n",
    "    pd.read_csv(\"../data/splits/train.csv\")\n",
    "      .dropna(subset=[\"question1\",\"question2\"])\n",
    ")\n",
    "train_df[\"qid1\"] = train_df[\"question1\"].map(qid_lookup).astype(int)\n",
    "train_df[\"qid2\"] = train_df[\"question2\"].map(qid_lookup).astype(int)\n",
    "\n",
    "if train_df[\"qid1\"].isna().any() or train_df[\"qid2\"].isna().any():\n",
    "    raise ValueError(\"Some questions in train.csv could not be mapped to question_meta.csv.\")\n",
    "\n",
    "# Build + fit IncrementalPCA (two passes) on TRAIN\n",
    "print(\"* Building TRAIN features + fitting IncrementalPCA (95% variance)…\")\n",
    "X_train = build_features(\n",
    "    pair_df         = train_df,\n",
    "    clean_questions = clean_questions,\n",
    "    meta_df         = meta_df,\n",
    "    embedding_path  = \"../data/processed/question_embeddings.npy\",\n",
    "    cache_dir       = str(MODEL_DIR),\n",
    "    cross_cache     = \"../data/processed/train_cross_scores.npy\",\n",
    "    fit_pca         = True        # ← fit & save PCA on TRAIN\n",
    ")\n",
    "np.save(PROCESSED_DIR / \"X_train.npy\", X_train)\n",
    "print(f\"* Saved X_train.npy with shape {X_train.shape} (d_reduced ≪ 3598)\")\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 5) BUILD & SAVE PCA-REDUCED FEATURES FOR VALID SPLIT\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "valid_df = pd.read_csv(\"../data/splits/valid.csv\").dropna(subset=[\"question1\",\"question2\"])\n",
    "valid_df[\"qid1\"] = valid_df[\"question1\"].map(qid_lookup).astype(int)\n",
    "valid_df[\"qid2\"] = valid_df[\"question2\"].map(qid_lookup).astype(int)\n",
    "\n",
    "if valid_df[\"qid1\"].isna().any() or valid_df[\"qid2\"].isna().any():\n",
    "    raise ValueError(\"Some questions in valid.csv could not be mapped to question_meta.csv.\")\n",
    "\n",
    "print(\"- Building VALID features + applying saved PCA…\")\n",
    "X_valid = build_features(\n",
    "    pair_df         = valid_df,\n",
    "    clean_questions = clean_questions,\n",
    "    meta_df         = meta_df,\n",
    "    embedding_path  = \"../data/processed/question_embeddings.npy\",\n",
    "    cache_dir       = str(MODEL_DIR),\n",
    "    cross_cache     = \"../data/processed/valid_cross_scores.npy\",\n",
    "    fit_pca         = False       # ← just transform with saved PCA\n",
    ")\n",
    "np.save(PROCESSED_DIR / \"X_valid.npy\", X_valid)\n",
    "print(f\"* Saved X_valid.npy with shape {X_valid.shape}\")\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 6) BUILD & SAVE PCA-REDUCED FEATURES FOR TEST SPLIT (optional)\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "test_df = pd.read_csv(\"../data/splits/test.csv\").dropna(subset=[\"question1\",\"question2\"])\n",
    "test_df[\"qid1\"] = test_df[\"question1\"].map(qid_lookup).astype(int)\n",
    "test_df[\"qid2\"] = test_df[\"question2\"].map(qid_lookup).astype(int)\n",
    "\n",
    "if test_df[\"qid1\"].isna().any() or test_df[\"qid2\"].isna().any():\n",
    "    raise ValueError(\"Some questions in test.csv could not be mapped to question_meta.csv.\")\n",
    "\n",
    "print(\"* Building TEST features + applying saved PCA…\")\n",
    "X_test = build_features(\n",
    "    pair_df         = test_df,\n",
    "    clean_questions = clean_questions,\n",
    "    meta_df         = meta_df,\n",
    "    embedding_path  = \"../data/processed/question_embeddings.npy\",\n",
    "    cache_dir       = str(MODEL_DIR),\n",
    "    cross_cache     = \"../data/processed/test_cross_scores.npy\",\n",
    "    fit_pca         = False       # ← just transform with saved PCA\n",
    ")\n",
    "np.save(PROCESSED_DIR / \"X_test.npy\", X_test)\n",
    "print(f\"* Saved X_test.npy with shape {X_test.shape}\")\n",
    "\n",
    "print(\"\\nFeature‐engineering + IncrementalPCA complete. Files now in data/processed:\")\n",
    "print(\"  * X_train.npy (TRAIN reduced → 95%)\")\n",
    "print(\"  * X_valid.npy (VALID reduced)\")\n",
    "print(\"  * X_test.npy  (TEST reduced)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
