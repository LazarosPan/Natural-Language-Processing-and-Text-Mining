{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfe580f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pairs to process: 323,554\n",
      "Feature matrix built. Shape = (323554, 1559)\n",
      "Saved: ../data/processed/X_train.npy\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# notebooks/03_feature_engineering.ipynb\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# 0. Allow importing from src/\n",
    "%run setup.py\n",
    "\n",
    "# 1. Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from src.features import build_features\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 2. Load preprocessing artefacts\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# Meta CSV (contains: question, clean, len, words)\n",
    "meta = pd.read_csv(\"../data/processed/question_meta.csv\")\n",
    "\n",
    "# Cleaned questions array (indexed by numeric ID = row in `meta`)\n",
    "clean = np.load(\"../data/processed/clean_questions.npy\", allow_pickle=True)\n",
    "\n",
    "# Sentence embeddings (384-d MiniLM vectors, indexed by the same ID)\n",
    "# We will pass this path to build_features\n",
    "embedding_path = \"../data/processed/question_embeddings.npy\"\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 3. Build a lookup from original question text → numeric ID\n",
    "#    so we can assign `qid1`/`qid2` in the pairs DataFrame\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# meta[\"question\"] holds the original text. Its row index (0..n-1) is the question ID.\n",
    "qid_lookup = {q: idx for idx, q in enumerate(meta[\"question\"].tolist())}\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 4. Load the train split and map qid1/qid2\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "train = pd.read_csv(\"../data/splits/train.csv\")\n",
    "# Drop any rows where either question is missing (just in case)\n",
    "train = train.dropna(subset=[\"question1\", \"question2\"])\n",
    "\n",
    "# Map the original question text to its numeric ID in `meta`\n",
    "train[\"qid1\"] = train[\"question1\"].map(qid_lookup)\n",
    "train[\"qid2\"] = train[\"question2\"].map(qid_lookup)\n",
    "\n",
    "# (Optional sanity‐check: ensure no NaNs remain in qid1/qid2)\n",
    "if train[\"qid1\"].isnull().any() or train[\"qid2\"].isnull().any():\n",
    "    missing_qs = train[train[\"qid1\"].isnull() | train[\"qid2\"].isnull()]\n",
    "    raise ValueError(\n",
    "        \"Some questions in train.csv did not appear in question_meta.csv:\\n\"\n",
    "        f\"{missing_qs.head(5)}\"\n",
    "    )\n",
    "\n",
    "# Convert qid1/qid2 to integer dtype\n",
    "train[\"qid1\"] = train[\"qid1\"].astype(int)\n",
    "train[\"qid2\"] = train[\"qid2\"].astype(int)\n",
    "\n",
    "print(f\"Total pairs to process: {len(train):,}\")\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 5. Build the full feature matrix for train\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# build_features signature:\n",
    "#   build_features(pair_df, clean_questions, meta_df,\n",
    "#                  embedding_path, cache_dir=\"models\") -> np.ndarray\n",
    "\n",
    "X_train = build_features(\n",
    "    pair_df = train,\n",
    "    clean_questions = clean.tolist(),      # list[str] of length = # unique questions\n",
    "    meta_df = meta,\n",
    "    embedding_path = embedding_path,\n",
    "    cache_dir = \"../models\"                # TF‐free cache of TF-IDF etc.\n",
    ")\n",
    "\n",
    "print(f\"Feature matrix built. Shape = {X_train.shape}\")\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 6. Save the feature matrix\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "OUT = Path(\"../data/processed\")\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "np.save(OUT/\"X_train.npy\", X_train)\n",
    "\n",
    "print(\"Saved: ../data/processed/X_train.npy\")\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# End of notebook.\n",
    "# ─────────────────────────────────────────────────────────────────────────────"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
