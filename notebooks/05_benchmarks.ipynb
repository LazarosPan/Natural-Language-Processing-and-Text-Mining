{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace3f771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 05_benchmarks.ipynb\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# 0) Ensure src/ is on PYTHONPATH, and import necessary pieces\n",
    "%run setup.py\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    log_loss,\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from src.pretrained_models import default_experts, MoEClassifier\n",
    "from src.logs import log_event, LogKind\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 1) Find all “moe_<key>_idxs.npy” under models/gates/ and pair with\n",
    "#    the matching “gate_<key>_retrained.pt”\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "gate_dir = Path(\"models/gates\")\n",
    "idx_files = sorted(gate_dir.glob(\"moe_*_idxs.npy\"))\n",
    "\n",
    "if not idx_files:\n",
    "    raise RuntimeError(f\"No ‘moe_<key>_idxs.npy’ files found in {gate_dir}.\")\n",
    "\n",
    "# Build a list of (key, idx_path, gate_path) tuples\n",
    "top_runs = []\n",
    "for idx_path in idx_files:\n",
    "    stem = idx_path.stem  # e.g. \"moe_LRFeatureExpert+XGBFeatureExpert_idxs\"\n",
    "    if not stem.startswith(\"moe_\") or not stem.endswith(\"_idxs\"):\n",
    "        continue\n",
    "    key = stem[len(\"moe_\") : -len(\"_idxs\")]  # e.g. \"LRFeatureExpert+XGBFeatureExpert\"\n",
    "    gate_path = gate_dir / f\"gate_{key}_retrained.pt\"\n",
    "    if not gate_path.exists():\n",
    "        continue  # skip if no matching .pt\n",
    "    top_runs.append((key, idx_path, gate_path))\n",
    "\n",
    "if not top_runs:\n",
    "    raise RuntimeError(f\"No matching ‘gate_<key>_retrained.pt’ found under {gate_dir}.\")\n",
    "\n",
    "print(f\"Found {len(top_runs)} gate checkpoints:\")\n",
    "for key, idx_path, gate_path in top_runs:\n",
    "    print(f\"  * key = {key!r}, idxs = {idx_path.name}, gate = {gate_path.name}\")\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 2) Prepare the TEST split once\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "test_df = pd.read_csv(\"../data/splits/test.csv\").dropna(subset=[\"question1\", \"question2\"])\n",
    "pairs_test = list(zip(test_df.question1, test_df.question2))\n",
    "y_test = test_df.is_duplicate.values.astype(int)\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 3) Evaluate each “top” run on TEST, collect & log metrics\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# We need a 768-dim embedding for QuoraDistilExpert\n",
    "emb_path_768 = \"../data/processed/question_embeddings_768.npy\"\n",
    "lr_path      = \"../models/pretrained/quoradistil_lr.pkl\"\n",
    "\n",
    "for key, idx_path, gate_path in top_runs:\n",
    "    # Load the selected expert-indices\n",
    "    idxs = np.load(idx_path).tolist()  # e.g. [0,3,5,...]\n",
    "\n",
    "    # Reconstruct the exact expert list (same order) and pick only those indices\n",
    "    all_experts = default_experts(\n",
    "        emb_path=emb_path_768,\n",
    "        lr_path=lr_path\n",
    "    )\n",
    "    experts = [all_experts[i] for i in idxs]\n",
    "\n",
    "    # Instantiate MoEClassifier (gate) and load saved weights\n",
    "    moe = MoEClassifier(experts, lr=1.0, epochs=0)\n",
    "    moe.gate.load_state_dict(torch.load(gate_path, map_location=\"cpu\"))\n",
    "    moe.gate.eval()\n",
    "\n",
    "    # Run inference on TEST and time it\n",
    "    t0 = time.time()\n",
    "    probs = moe.predict_prob(pairs_test)\n",
    "    inference_time = time.time() - t0\n",
    "\n",
    "    # Compute thresholded predictions\n",
    "    preds = (probs > 0.5).astype(int)\n",
    "\n",
    "    # Compute a variety of metrics\n",
    "    ll   = log_loss(y_test, probs)\n",
    "    acc  = accuracy_score(y_test, preds)\n",
    "    f1   = f1_score(y_test, preds)\n",
    "    prec = precision_score(y_test, preds)\n",
    "    rec  = recall_score(y_test, preds)\n",
    "\n",
    "    # ROC-AUC requires probabilities and both classes present\n",
    "    try:\n",
    "        auc = roc_auc_score(y_test, probs)\n",
    "    except ValueError:\n",
    "        auc = float(\"nan\")\n",
    "\n",
    "    # Print to notebook\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"Gate key: {key}\")\n",
    "    print(f\"  * TEST log-loss  = {ll:.4f}\")\n",
    "    print(f\"  * TEST accuracy  = {acc:.4f}\")\n",
    "    print(f\"  * TEST  F1 score = {f1:.4f}\")\n",
    "    print(f\"  * TEST Precision = {prec:.4f}\")\n",
    "    print(f\"  * TEST   Recall  = {rec:.4f}\")\n",
    "    print(f\"  * TEST    AUC    = {auc:.4f}\")\n",
    "    print(f\"  * Inference time = {inference_time:.2f}s\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Log to metric_logs/benchmarks.csv\n",
    "    log_event(\n",
    "        LogKind.TEST,\n",
    "        model=key,\n",
    "        phase=\"eval\",\n",
    "        seconds=round(inference_time, 2),\n",
    "        test_LL=round(ll, 6),\n",
    "        test_ACC=round(acc, 4),\n",
    "        test_F1=round(f1, 4),\n",
    "        test_PREC=round(prec, 4),\n",
    "        test_REC=round(rec, 4),\n",
    "        test_AUC=round(auc, 4),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb941f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1) Load the benchmark CSV\n",
    "df = pd.read_csv(\"metric_logs/benchmarks.csv\")\n",
    "\n",
    "# 2) Show the full DataFrame for inspection\n",
    "import ace_tools as tools\n",
    "tools.display_dataframe_to_user(name=\"Benchmark Results\", dataframe=df)\n",
    "\n",
    "# 3) Display summary statistics\n",
    "summary = df.describe().round(4)\n",
    "tools.display_dataframe_to_user(name=\"Summary Statistics\", dataframe=summary)\n",
    "\n",
    "# 4) Bar plot: Test Log-Loss by Model\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(df[\"model\"], df[\"test_LL\"])\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"Test Log-Loss\")\n",
    "plt.title(\"Test Log-Loss by Model\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 5) Bar plot: Test Accuracy by Model\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(df[\"model\"], df[\"test_ACC\"])\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"Test Accuracy\")\n",
    "plt.title(\"Test Accuracy by Model\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 6) Bar plot: Test F1 Score by Model\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(df[\"model\"], df[\"test_F1\"])\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"Test F1 Score\")\n",
    "plt.title(\"Test F1 Score by Model\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 7) Scatter: Inference Time vs Test Log-Loss\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(df[\"seconds\"], df[\"test_LL\"])\n",
    "plt.xlabel(\"Inference Time (s)\")\n",
    "plt.ylabel(\"Test Log-Loss\")\n",
    "plt.title(\"Inference Time vs Test Log-Loss\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 8) Correlation matrix heatmap of metrics\n",
    "corr = df[\n",
    "    [\"test_LL\", \"test_ACC\", \"test_F1\", \"test_PREC\", \"test_REC\", \"test_AUC\", \"seconds\"]\n",
    "].corr()\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(corr, cmap=\"viridis\", interpolation=\"none\")\n",
    "plt.colorbar()\n",
    "plt.xticks(range(len(corr)), corr.columns, rotation=45, ha=\"right\")\n",
    "plt.yticks(range(len(corr)), corr.columns)\n",
    "plt.title(\"Correlation Matrix of Metrics\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
