{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9e4bda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x798ffb5e58d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ────────────────────────────────────────────────────────────────\n",
    "# 04_models.ipynb  ·  Mixture‐of‐Experts training & automated tuning\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "# ── CELL 1 ───────────────────────────────────────────────────────────────────\n",
    "# 0) Ensure src/ is on PYTHONPATH\n",
    "%run setup.py\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from itertools import combinations\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 13\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "081434f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 1) Load train/valid splits\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "DATA_DIR = Path(\"../data/splits\")\n",
    "train_df = pd.read_csv(DATA_DIR / \"train.csv\").dropna(subset=[\"question1\", \"question2\"])\n",
    "valid_df = pd.read_csv(DATA_DIR / \"valid.csv\").dropna(subset=[\"question1\", \"question2\"])\n",
    "\n",
    "pairs_tr   = list(zip(train_df.question1, train_df.question2))\n",
    "y_tr       = train_df.is_duplicate.values.astype(int)\n",
    "pairs_val  = list(zip(valid_df.question1, valid_df.question2))\n",
    "y_val      = valid_df.is_duplicate.values.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a386e5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 2) Ensure necessary directories exist\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# pretrained models (e.g. QuoraDistilExpert's LR pickle)\n",
    "PRETRAINED_DIR = Path(\"../models/pretrained\")\n",
    "PRETRAINED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# custom models (feature-based pickles)\n",
    "CUSTOM_DIR = Path(\"../models/custom\")\n",
    "CUSTOM_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# feature artifacts (TF-IDF & SVD pickles)\n",
    "FEATURES_DIR = Path(\"../models/features\")\n",
    "FEATURES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# MoE gate checkpoints\n",
    "GATE_DIR = Path(\"../models/gates\")\n",
    "GATE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Timing log\n",
    "TIMING_LOG = Path(\"../models/timing_log.txt\")\n",
    "if not TIMING_LOG.exists():\n",
    "    with open(TIMING_LOG, \"w\") as f:\n",
    "        f.write(\"subset_key\\tstatus\\ttrain_or_load_time(s)\\tvalidation_time(s)\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "437514f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 3) Import all experts\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "from src.pretrained_models import (\n",
    "    BertExpert,\n",
    "    RobertaExpert,\n",
    "    XLNetExpert,\n",
    "    QuoraDistilExpert,\n",
    "    CrossEncExpert,\n",
    ")\n",
    "from src.custom_models import (\n",
    "    LRFeatureExpert,\n",
    "    XGBFeatureExpert,\n",
    "    LGBMFeatureExpert,\n",
    ")\n",
    "from src.pretrained_models import MoEClassifier, get_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5437f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Initializing classical feature-based experts…\n",
      "   * LRFeatureExpert pickle found—skipping training.\n",
      "   * XGBFeatureExpert pickle found—skipping training.\n",
      "   * LGBMFeatureExpert pickle found—skipping training.\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 4) Instantiate & (if needed) fit feature-based experts\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "print(\">>> Initializing classical feature-based experts…\")\n",
    "\n",
    "# 4a) Prepare a version of train_df with qid1/qid2 (for feature_experts.fit)\n",
    "meta = pd.read_csv(\"../data/processed/question_meta.csv\")\n",
    "rev  = {q: i for i, q in enumerate(meta.question)}\n",
    "# assign qid1, qid2 in-place\n",
    "train_df = train_df.assign(\n",
    "    qid1=lambda d: d.question1.map(rev).astype(int),\n",
    "    qid2=lambda d: d.question2.map(rev).astype(int),\n",
    ")\n",
    "\n",
    "# LRFeatureExpert\n",
    "lr_exp = LRFeatureExpert()\n",
    "if not lr_exp.model_path.exists():\n",
    "    print(\"   * Fitting LRFeatureExpert on engineered features…\")\n",
    "    lr_exp.fit(train_df, y_tr)\n",
    "else:\n",
    "    print(\"   * LRFeatureExpert pickle found—skipping training.\")\n",
    "\n",
    "# XGBFeatureExpert\n",
    "xgb_exp = XGBFeatureExpert()\n",
    "if not xgb_exp.model_path.exists():\n",
    "    print(\"   * Fitting XGBFeatureExpert on engineered features…\")\n",
    "    xgb_exp.fit(train_df, y_tr)\n",
    "else:\n",
    "    print(\"   * XGBFeatureExpert pickle found—skipping training.\")\n",
    "\n",
    "# LGBMFeatureExpert\n",
    "lgb_exp = LGBMFeatureExpert()\n",
    "if not lgb_exp.model_path.exists():\n",
    "    print(\"   * Fitting LGBMFeatureExpert on engineered features…\")\n",
    "    lgb_exp.fit(train_df, y_tr)\n",
    "else:\n",
    "    print(\"   * LGBMFeatureExpert pickle found—skipping training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3542949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Initializing Hugging-Face experts…\n",
      "   * Skipping XLNetExpert (sentencepiece not installed).\n",
      "   * HF experts = ['BertExpert', 'RobertaExpert', 'QuoraDistilExpert', 'CrossEncExpert']\n",
      "   * QuoraDistilExpert LR already present—skipping LR training.\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 5) Instantiate Hugging-Face experts\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "print(\"\\n>>> Initializing Hugging-Face experts…\")\n",
    "\n",
    "# Ensure QuoraDistilExpert’s LR path is under models/pretrained/\n",
    "EMB_PATH = \"../data/processed/question_embeddings.npy\"\n",
    "LR_PATH  = PRETRAINED_DIR / \"quoradistil_lr.pkl\"\n",
    "\n",
    "# Instantiate each HF expert.  If QuoraDistilExpert’s LR is missing, we'll fit below.\n",
    "hf_experts = [\n",
    "    BertExpert(),\n",
    "    RobertaExpert(),\n",
    "    # Try to include XLNet if sentencepiece is installed\n",
    "]\n",
    "try:\n",
    "    xl = XLNetExpert()\n",
    "    hf_experts.append(xl)\n",
    "except RuntimeError as e:\n",
    "    print(\"   * Skipping XLNetExpert (sentencepiece not installed).\")\n",
    "\n",
    "# QuoraDistilExpert\n",
    "quora_exp = QuoraDistilExpert(\n",
    "    emb_path=EMB_PATH,\n",
    "    lr_path=str(LR_PATH),\n",
    ")\n",
    "hf_experts.append(quora_exp)\n",
    "\n",
    "# CrossEncExpert\n",
    "cross_exp = CrossEncExpert()\n",
    "hf_experts.append(cross_exp)\n",
    "\n",
    "print(f\"   * HF experts = {[e.__class__.__name__ for e in hf_experts]}\")\n",
    "\n",
    "# 5a) Train QuoraDistilExpert’s LR if missing\n",
    "if not quora_exp.lr_path.exists():\n",
    "    print(\"   * QuoraDistilExpert LR not found; training fresh LogisticRegression…\")\n",
    "    # We already have train_df[qid1], train_df[qid2]\n",
    "    t0 = time.time()\n",
    "    quora_exp.fit(\n",
    "        train_df.qid1.values.astype(int),\n",
    "        train_df.qid2.values.astype(int),\n",
    "        y_tr\n",
    "    )\n",
    "    elapsed = time.time() - t0\n",
    "    print(f\"     -> QuoraDistilExpert LR trained in {elapsed:.1f}s.\")\n",
    "else:\n",
    "    print(\"   * QuoraDistilExpert LR already present—skipping LR training.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9da12ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total experts = 7:\n",
      "   – BertExpert\n",
      "   – RobertaExpert\n",
      "   – QuoraDistilExpert\n",
      "   – CrossEncExpert\n",
      "   – LRFeatureExpert\n",
      "   – XGBFeatureExpert\n",
      "   – LGBMFeatureExpert\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 6) Combine all experts into one list\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "experts = hf_experts + [lr_exp, xgb_exp, lgb_exp]\n",
    "print(f\"\\nTotal experts = {len(experts)}:\")\n",
    "for e in experts:\n",
    "    print(\"   –\", e.__class__.__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9154221a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached P_tr & P_val (skipping forward‐passes).\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 7) Pre-compute & cache expert outputs on train/valid\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "from sklearn.metrics import log_loss\n",
    "import os, glob\n",
    "\n",
    "# If all 7 expert‐files exist, just load them instead of calling get_predictions.\n",
    "pred_files = glob.glob(\"../models/pred_cache/train_*.npy\")\n",
    "if len(pred_files) == len(experts):\n",
    "    print(\"Loading cached P_tr & P_val (skipping forward‐passes).\")\n",
    "    P_tr = np.column_stack([np.load(f, mmap_mode=\"r\") for f in sorted(pred_files)])\n",
    "    pred_files_val = glob.glob(\"../models/pred_cache/valid_*.npy\")\n",
    "    P_val = np.column_stack([np.load(f, mmap_mode=\"r\") for f in sorted(pred_files_val)])\n",
    "else:\n",
    "    print(\"\\n>>> Pre-computing predictions for each expert on train/valid splits…\")\n",
    "    t0 = time.time()\n",
    "    P_tr  = get_predictions(experts, pairs_tr,  \"train\")\n",
    "    P_val = get_predictions(experts, pairs_val, \"valid\")\n",
    "    elapsed = time.time() - t0\n",
    "    print(f\"   * Expert forward-passes cached in {elapsed:.1f}s.  Shapes: {P_tr.shape}, {P_val.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2ea3829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Starting gate tuning over valid split…\n",
      "   * Evaluating 127 distinct subsets…\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      ">> Subset (0,) (BertExpert) -> gate loaded in 0.0s\n",
      "--------------------------------------------------------------------------------\n",
      "   valid log-loss = 0.1025  (eval took 35.6s)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      ">> Subset (1,) (RobertaExpert) -> gate loaded in 0.0s\n",
      "--------------------------------------------------------------------------------\n",
      "   valid log-loss = 0.0571  (eval took 92.9s)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      ">> Subset (2,) (QuoraDistilExpert) -> gate loaded in 0.0s\n",
      "--------------------------------------------------------------------------------\n",
      "   valid log-loss = 0.2960  (eval took 9.5s)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      ">> Subset (3,) (CrossEncExpert) -> gate loaded in 0.0s\n",
      "--------------------------------------------------------------------------------\n",
      "   valid log-loss = 0.2393  (eval took 89.7s)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      ">> Subset (4,) (LRFeatureExpert) -> training gate…\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /api/models/cross-encoder/quora-roberta-large/tree/main/additional_chat_templates?recursive=False&expand=False (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x798cca5ddd10>: Failed to resolve \\'huggingface.co\\' ([Errno -3] Temporary failure in name resolution)\"))'), '(Request ID: 2946f141-d609-4093-9731-2a8051885920)')",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mgaierror\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.13/site-packages/urllib3/connection.py:198\u001b[39m, in \u001b[36mHTTPConnection._new_conn\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m     sock = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m socket.gaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.13/site-packages/urllib3/util/connection.py:60\u001b[39m, in \u001b[36mcreate_connection\u001b[39m\u001b[34m(address, timeout, source_address, socket_options)\u001b[39m\n\u001b[32m     58\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m LocationParseError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhost\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, label empty or too long\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43msocket\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msocket\u001b[49m\u001b[43m.\u001b[49m\u001b[43mSOCK_STREAM\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m     61\u001b[39m     af, socktype, proto, canonname, sa = res\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.13/socket.py:977\u001b[39m, in \u001b[36mgetaddrinfo\u001b[39m\u001b[34m(host, port, family, type, proto, flags)\u001b[39m\n\u001b[32m    976\u001b[39m addrlist = []\n\u001b[32m--> \u001b[39m\u001b[32m977\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_socket\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    978\u001b[39m     af, socktype, proto, canonname, sa = res\n",
      "\u001b[31mgaierror\u001b[39m: [Errno -3] Temporary failure in name resolution",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mNameResolutionError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.13/site-packages/urllib3/connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.13/site-packages/urllib3/connectionpool.py:488\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    487\u001b[39m         new_e = _wrap_proxy_error(new_e, conn.proxy.scheme)\n\u001b[32m--> \u001b[39m\u001b[32m488\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m new_e\n\u001b[32m    490\u001b[39m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[32m    491\u001b[39m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.13/site-packages/urllib3/connectionpool.py:464\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    463\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m464\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    465\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.13/site-packages/urllib3/connectionpool.py:1093\u001b[39m, in \u001b[36mHTTPSConnectionPool._validate_conn\u001b[39m\u001b[34m(self, conn)\u001b[39m\n\u001b[32m   1092\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m conn.is_closed:\n\u001b[32m-> \u001b[39m\u001b[32m1093\u001b[39m     \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1095\u001b[39m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.13/site-packages/urllib3/connection.py:704\u001b[39m, in \u001b[36mHTTPSConnection.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    703\u001b[39m sock: socket.socket | ssl.SSLSocket\n\u001b[32m--> \u001b[39m\u001b[32m704\u001b[39m \u001b[38;5;28mself\u001b[39m.sock = sock = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m server_hostname: \u001b[38;5;28mstr\u001b[39m = \u001b[38;5;28mself\u001b[39m.host\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.13/site-packages/urllib3/connection.py:205\u001b[39m, in \u001b[36mHTTPConnection._new_conn\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m socket.gaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m NameResolutionError(\u001b[38;5;28mself\u001b[39m.host, \u001b[38;5;28mself\u001b[39m, e) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mNameResolutionError\u001b[39m: <urllib3.connection.HTTPSConnection object at 0x798cca5ddd10>: Failed to resolve 'huggingface.co' ([Errno -3] Temporary failure in name resolution)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mMaxRetryError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.13/site-packages/requests/adapters.py:667\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m667\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.13/site-packages/urllib3/connectionpool.py:841\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    839\u001b[39m     new_e = ProtocolError(\u001b[33m\"\u001b[39m\u001b[33mConnection aborted.\u001b[39m\u001b[33m\"\u001b[39m, new_e)\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m retries = \u001b[43mretries\u001b[49m\u001b[43m.\u001b[49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m=\u001b[49m\u001b[43msys\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    844\u001b[39m retries.sleep()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.13/site-packages/urllib3/util/retry.py:519\u001b[39m, in \u001b[36mRetry.increment\u001b[39m\u001b[34m(self, method, url, response, error, _pool, _stacktrace)\u001b[39m\n\u001b[32m    518\u001b[39m     reason = error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[32m--> \u001b[39m\u001b[32m519\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    521\u001b[39m log.debug(\u001b[33m\"\u001b[39m\u001b[33mIncremented Retry for (url=\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m, url, new_retry)\n",
      "\u001b[31mMaxRetryError\u001b[39m: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /api/models/cross-encoder/quora-roberta-large/tree/main/additional_chat_templates?recursive=False&expand=False (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x798cca5ddd10>: Failed to resolve 'huggingface.co' ([Errno -3] Temporary failure in name resolution)\"))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mConnectionError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 42\u001b[39m\n\u001b[32m     40\u001b[39m moe = MoEClassifier(subset_exps, lr=\u001b[32m1e-2\u001b[39m, epochs=\u001b[32m2\u001b[39m)\n\u001b[32m     41\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m>> Subset \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midxs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m+\u001b[39m\u001b[33m'\u001b[39m.join(subset_names)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) -> training gate…\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m \u001b[43mmoe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpairs_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_tr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m train_time = time.time() - t1\n\u001b[32m     44\u001b[39m save_gate(moe, ckpt_path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/1) Μεταπτυχιακό/2nd Semester/NLP/Natural-Language-Processing-and-Text-Mining/src/pretrained_models.py:407\u001b[39m, in \u001b[36mMoEClassifier.fit\u001b[39m\u001b[34m(self, pairs, y)\u001b[39m\n\u001b[32m    404\u001b[39m \u001b[38;5;66;03m# 1) Freeze experts, collect their predictions (no grad):\u001b[39;00m\n\u001b[32m    405\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m    406\u001b[39m     probs = np.column_stack(\n\u001b[32m--> \u001b[39m\u001b[32m407\u001b[39m         [\u001b[43mexp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_pairs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m exp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.experts]\n\u001b[32m    408\u001b[39m     )  \u001b[38;5;66;03m# shape: (batch_size, K)\u001b[39;00m\n\u001b[32m    409\u001b[39m     probs_t = torch.tensor(probs, dtype=torch.float32).to(_DEVICE)\n\u001b[32m    411\u001b[39m \u001b[38;5;66;03m# 2) Gate forward:\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/1) Μεταπτυχιακό/2nd Semester/NLP/Natural-Language-Processing-and-Text-Mining/src/pretrained_models.py:95\u001b[39m, in \u001b[36mBaseExpert.predict_prob\u001b[39m\u001b[34m(self, pairs)\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     89\u001b[39m \u001b[33;03mInput : list[(q₁, q₂)] (raw text strings)\u001b[39;00m\n\u001b[32m     90\u001b[39m \u001b[33;03mOutput: 1-D numpy array of shape (len(pairs),), dtype=float32 ∈ [0,1].\u001b[39;00m\n\u001b[32m     91\u001b[39m \n\u001b[32m     92\u001b[39m \u001b[33;03mInternally, it calls `_predict_impl` in minibatches of size `batch_size`.\u001b[39;00m\n\u001b[32m     93\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     94\u001b[39m \u001b[38;5;28mself\u001b[39m._ensure_loaded()\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predict_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpairs\u001b[49m\u001b[43m)\u001b[49m.astype(\u001b[33m\"\u001b[39m\u001b[33mfloat32\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/1) Μεταπτυχιακό/2nd Semester/NLP/Natural-Language-Processing-and-Text-Mining/src/custom_models.py:175\u001b[39m, in \u001b[36mFeatureExpert._predict_impl\u001b[39m\u001b[34m(self, pairs)\u001b[39m\n\u001b[32m    171\u001b[39m pair_df = _pairs_to_df(pairs)\n\u001b[32m    173\u001b[39m \u001b[38;5;66;03m# 2) Build the full feature matrix (shape: n_pairs × 3 598),\u001b[39;00m\n\u001b[32m    174\u001b[39m \u001b[38;5;66;03m#    then PCA-reduce internally (95% variance).\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m X = \u001b[43mbuild_features\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpair_df\u001b[49m\u001b[43m         \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mpair_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclean_questions\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m_clean_qs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmeta_df\u001b[49m\u001b[43m         \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m_meta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_path\u001b[49m\u001b[43m  \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m_emb_fp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m       \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m../models/features\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# TF-IDF & SVD pickles live here\u001b[39;49;00m\n\u001b[32m    181\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcross_cache\u001b[49m\u001b[43m     \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m                        \u001b[49m\u001b[38;5;66;43;03m# compute cross-encoder on the fly\u001b[39;49;00m\n\u001b[32m    182\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[38;5;66;03m# 3) Use underlying classifier to produce P(duplicate)\u001b[39;00m\n\u001b[32m    185\u001b[39m prob = \u001b[38;5;28mself\u001b[39m.clf.predict_proba(X)[:, \u001b[32m1\u001b[39m].astype(\u001b[33m\"\u001b[39m\u001b[33mfloat32\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/1) Μεταπτυχιακό/2nd Semester/NLP/Natural-Language-Processing-and-Text-Mining/src/features.py:377\u001b[39m, in \u001b[36mbuild_features\u001b[39m\u001b[34m(pair_df, clean_questions, meta_df, embedding_path, cache_dir, cross_cache, fit_pca)\u001b[39m\n\u001b[32m    372\u001b[39m numeric_block = _numeric(meta_df, idx1, idx2, freq_arr)  \u001b[38;5;66;03m# (n_pairs, 18)\u001b[39;00m\n\u001b[32m    374\u001b[39m \u001b[38;5;66;03m# ───────────────────────────────────────────────────────────────────\u001b[39;00m\n\u001b[32m    375\u001b[39m \u001b[38;5;66;03m# G. CROSS-ENCODER PROBABILITIES\u001b[39;00m\n\u001b[32m    376\u001b[39m \u001b[38;5;66;03m# ───────────────────────────────────────────────────────────────────\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m377\u001b[39m cross_block = \u001b[43m_cross_scores\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpair_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcross-encoder/quora-roberta-large\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    381\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcross_cache\u001b[49m\n\u001b[32m    382\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m.reshape(-\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# (n_pairs, 1)\u001b[39;00m\n\u001b[32m    384\u001b[39m \u001b[38;5;66;03m# ───────────────────────────────────────────────────────────────────\u001b[39;00m\n\u001b[32m    385\u001b[39m \u001b[38;5;66;03m# CONCAT raw 3 598-dim features\u001b[39;00m\n\u001b[32m    386\u001b[39m \u001b[38;5;66;03m# ───────────────────────────────────────────────────────────────────\u001b[39;00m\n\u001b[32m    387\u001b[39m X_raw = np.hstack([\n\u001b[32m    388\u001b[39m     cos_block,      \u001b[38;5;66;03m# 2\u001b[39;00m\n\u001b[32m    389\u001b[39m     dense_block,    \u001b[38;5;66;03m# 3072\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    394\u001b[39m     svd_block       \u001b[38;5;66;03m# 500\u001b[39;00m\n\u001b[32m    395\u001b[39m ]).astype(\u001b[33m\"\u001b[39m\u001b[33mfloat32\u001b[39m\u001b[33m\"\u001b[39m)  \u001b[38;5;66;03m# -> (n_pairs, 3598)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/1) Μεταπτυχιακό/2nd Semester/NLP/Natural-Language-Processing-and-Text-Mining/src/features.py:254\u001b[39m, in \u001b[36m_cross_scores\u001b[39m\u001b[34m(pair_df, model_name, batch_size, cache_file)\u001b[39m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m _CROSS_CACHE:\n\u001b[32m    253\u001b[39m     dev = \u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m254\u001b[39m     _CROSS_CACHE[model_name] = \u001b[43mCrossEncoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdev\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m        \u001b[49m\u001b[43mactivation_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mSigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    260\u001b[39m model  = _CROSS_CACHE[model_name]\n\u001b[32m    261\u001b[39m tuples = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(pair_df.question1.values, pair_df.question2.values))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/data-science/lib/python3.13/site-packages/sentence_transformers/cross_encoder/util.py:39\u001b[39m, in \u001b[36mcross_encoder_init_args_decorator.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     36\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     37\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mconfig_kwargs\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mclassifier_dropout\u001b[39m\u001b[33m\"\u001b[39m] = classifier_dropout\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/data-science/lib/python3.13/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:192\u001b[39m, in \u001b[36mCrossEncoder.__init__\u001b[39m\u001b[34m(self, model_name_or_path, num_labels, max_length, activation_fn, device, cache_folder, trust_remote_code, revision, local_files_only, token, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data, backend)\u001b[39m\n\u001b[32m    190\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mmodel_max_length\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m tokenizer_kwargs \u001b[38;5;129;01mand\u001b[39;00m max_length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    191\u001b[39m     tokenizer_kwargs[\u001b[33m\"\u001b[39m\u001b[33mmodel_max_length\u001b[39m\u001b[33m\"\u001b[39m] = max_length\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m \u001b[38;5;28mself\u001b[39m.tokenizer: PreTrainedTokenizer = \u001b[43mAutoTokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mmodel_max_length\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m tokenizer_kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.config, \u001b[33m\"\u001b[39m\u001b[33mmax_position_embeddings\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    202\u001b[39m     \u001b[38;5;28mself\u001b[39m.tokenizer.model_max_length = \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mself\u001b[39m.tokenizer.model_max_length, \u001b[38;5;28mself\u001b[39m.config.max_position_embeddings)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/data-science/lib/python3.13/site-packages/transformers/models/auto/tokenization_auto.py:1013\u001b[39m, in \u001b[36mAutoTokenizer.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[39m\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1010\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1011\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTokenizer class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer_class_candidate\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not exist or is not currently imported.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1012\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1013\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[38;5;66;03m# Otherwise we have to be creative.\u001b[39;00m\n\u001b[32m   1016\u001b[39m \u001b[38;5;66;03m# if model is an encoder decoder, the encoder tokenizer class is used by default\u001b[39;00m\n\u001b[32m   1017\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, EncoderDecoderConfig):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/data-science/lib/python3.13/site-packages/transformers/tokenization_utils_base.py:1968\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   1964\u001b[39m                         vocab_files[\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mchat_template_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemplate_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m] = (\n\u001b[32m   1965\u001b[39m                             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCHAT_TEMPLATE_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemplate_file.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1966\u001b[39m                         )\n\u001b[32m   1967\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1968\u001b[39m                 \u001b[38;5;28;01mfor\u001b[39;00m template \u001b[38;5;129;01min\u001b[39;00m \u001b[43mlist_repo_templates\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1969\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1970\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1971\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1972\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1973\u001b[39m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   1974\u001b[39m                     vocab_files[\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mchat_template_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemplate\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m] = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCHAT_TEMPLATE_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemplate\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.jinja\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1976\u001b[39m \u001b[38;5;66;03m# Get files from url, cache, or disk depending on the case\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/data-science/lib/python3.13/site-packages/transformers/utils/hub.py:163\u001b[39m, in \u001b[36mlist_repo_templates\u001b[39m\u001b[34m(repo_id, local_files_only, revision, cache_dir)\u001b[39m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m local_files_only:\n\u001b[32m    160\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    161\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m            \u001b[49m\u001b[43mentry\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mremoveprefix\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mCHAT_TEMPLATE_DIR\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m--> \u001b[39m\u001b[32m163\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mentry\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlist_repo_tree\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath_in_repo\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCHAT_TEMPLATE_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m    165\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mentry\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mendswith\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.jinja\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    168\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (GatedRepoError, RepositoryNotFoundError, RevisionNotFoundError):\n\u001b[32m    169\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m  \u001b[38;5;66;03m# valid errors => do not catch\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/data-science/lib/python3.13/site-packages/huggingface_hub/hf_api.py:3140\u001b[39m, in \u001b[36mHfApi.list_repo_tree\u001b[39m\u001b[34m(self, repo_id, path_in_repo, recursive, expand, revision, repo_type, token)\u001b[39m\n\u001b[32m   3138\u001b[39m encoded_path_in_repo = \u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m + quote(path_in_repo, safe=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m path_in_repo \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3139\u001b[39m tree_url = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.endpoint\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/api/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33ms/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/tree/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mencoded_path_in_repo\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m3140\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpath_info\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpaginate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtree_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrecursive\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mexpand\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpand\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3141\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mRepoFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpath_info\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpath_info\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtype\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfile\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mRepoFolder\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpath_info\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/data-science/lib/python3.13/site-packages/huggingface_hub/utils/_pagination.py:36\u001b[39m, in \u001b[36mpaginate\u001b[39m\u001b[34m(path, params, headers)\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Fetch a list of models/datasets/spaces and paginate through results.\u001b[39;00m\n\u001b[32m     29\u001b[39m \n\u001b[32m     30\u001b[39m \u001b[33;03mThis is using the same \"Link\" header format as GitHub.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     33\u001b[39m \u001b[33;03m- https://docs.github.com/en/rest/guides/traversing-with-pagination#link-header\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     35\u001b[39m session = get_session()\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m r = \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m hf_raise_for_status(r)\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01myield from\u001b[39;00m r.json()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.13/site-packages/requests/sessions.py:602\u001b[39m, in \u001b[36mSession.get\u001b[39m\u001b[34m(self, url, **kwargs)\u001b[39m\n\u001b[32m    594\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[32m    595\u001b[39m \n\u001b[32m    596\u001b[39m \u001b[33;03m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m    597\u001b[39m \u001b[33;03m:param \\*\\*kwargs: Optional arguments that ``request`` takes.\u001b[39;00m\n\u001b[32m    598\u001b[39m \u001b[33;03m:rtype: requests.Response\u001b[39;00m\n\u001b[32m    599\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    601\u001b[39m kwargs.setdefault(\u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m602\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mGET\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.13/site-packages/requests/sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.13/site-packages/requests/sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/data-science/lib/python3.13/site-packages/huggingface_hub/utils/_http.py:96\u001b[39m, in \u001b[36mUniqueRequestIdAdapter.send\u001b[39m\u001b[34m(self, request, *args, **kwargs)\u001b[39m\n\u001b[32m     94\u001b[39m     logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSend: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_curlify(request)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m requests.RequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     98\u001b[39m     request_id = request.headers.get(X_AMZN_TRACE_ID)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.13/site-packages/requests/adapters.py:700\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    696\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e.reason, _SSLError):\n\u001b[32m    697\u001b[39m         \u001b[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[32m    698\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request=request)\n\u001b[32m--> \u001b[39m\u001b[32m700\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request=request)\n\u001b[32m    702\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    703\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request=request)\n",
      "\u001b[31mConnectionError\u001b[39m: (MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /api/models/cross-encoder/quora-roberta-large/tree/main/additional_chat_templates?recursive=False&expand=False (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x798cca5ddd10>: Failed to resolve \\'huggingface.co\\' ([Errno -3] Temporary failure in name resolution)\"))'), '(Request ID: 2946f141-d609-4093-9731-2a8051885920)')"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 8) Gate tuning over valid split + disk-cache for every subset\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "from src.pretrained_models import save_gate, load_gate, _subset_key\n",
    "\n",
    "print(\"\\n>>> Starting gate tuning over valid split…\")\n",
    "# Build all non-empty subsets of expert indices\n",
    "idx_of = {e.__class__.__name__: i for i, e in enumerate(experts)}\n",
    "valid_subsets: list[tuple[int, ...]] = []\n",
    "for k in range(1, len(idx_of) + 1):\n",
    "    for tpl in combinations(idx_of.values(), k):\n",
    "        valid_subsets.append(tpl)\n",
    "\n",
    "print(f\"   * Evaluating {len(valid_subsets)} distinct subsets…\\n\")\n",
    "\n",
    "best_ll, best_subset = 1e9, None\n",
    "best_moe = None\n",
    "\n",
    "for idxs in valid_subsets:\n",
    "    subset_exps  = [experts[i] for i in idxs]\n",
    "    subset_names = [e.__class__.__name__ for e in subset_exps]\n",
    "    key = _subset_key(subset_exps)\n",
    "    ckpt_path = GATE_DIR / f\"gate_{key}.pt\"\n",
    "\n",
    "    start_all = time.time()\n",
    "\n",
    "    if ckpt_path.exists():\n",
    "        # a) load pre-trained gate\n",
    "        print(\"-\" * 80)\n",
    "        t1 = time.time()\n",
    "        moe = load_gate(subset_exps, ckpt_path)\n",
    "        load_time = time.time() - t1\n",
    "        status = \"loaded\"\n",
    "        print(f\"\\n>> Subset {idxs} ({'+'.join(subset_names)}) -> gate {status} in {load_time:.1f}s\")\n",
    "        print(\"-\" * 80)\n",
    "    else:\n",
    "        # b) train a new gate\n",
    "        print(\"-\" * 80)\n",
    "        t1 = time.time()\n",
    "        moe = MoEClassifier(subset_exps, lr=1e-2, epochs=2)\n",
    "        print(f\"\\n>> Subset {idxs} ({'+'.join(subset_names)}) -> training gate…\")\n",
    "        moe.fit(pairs_tr, y_tr)\n",
    "        train_time = time.time() - t1\n",
    "        save_gate(moe, ckpt_path)\n",
    "        status = \"trained\"\n",
    "        print(f\"   -> gate trained & cached in {train_time:.1f}s\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "    # Evaluate on validation\n",
    "    t2 = time.time()\n",
    "    ll = moe.evaluate(pairs_val, y_val)\n",
    "    val_time = time.time() - t2\n",
    "    print(f\"   valid log-loss = {ll:.4f}  (eval took {val_time:.1f}s)\\n\")\n",
    "\n",
    "    # Write to timing log\n",
    "    with open(TIMING_LOG, \"a\") as f:\n",
    "        if status == \"loaded\":\n",
    "            f.write(f\"{key}\\tloaded\\t{load_time:.1f}\\t{val_time:.1f}\\n\")\n",
    "        else:\n",
    "            f.write(f\"{key}\\ttrained\\t{train_time:.1f}\\t{val_time:.1f}\\n\")\n",
    "\n",
    "    total_time = time.time() - start_all\n",
    "    # Track best subset\n",
    "    if ll < best_ll:\n",
    "        best_ll, best_subset = ll, idxs\n",
    "        best_moe = moe\n",
    "\n",
    "print(f\"\\n>>> BEST subset {best_subset}  ·  valid LL = {best_ll:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafc99d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 9) Retrain BEST gate on Train+Valid & save final checkpoint\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "print(\"\\n>>> Re-training best gate on Train+Valid…\")\n",
    "pairs_tv = pairs_tr + pairs_val\n",
    "y_tv     = np.concatenate([y_tr, y_val])\n",
    "\n",
    "best_names = [experts[i].__class__.__name__ for i in best_subset]\n",
    "print(f\"   * Subset indices = {best_subset} ({'+'.join(best_names)})\")\n",
    "\n",
    "start_tv = time.time()\n",
    "final_gate = MoEClassifier([experts[i] for i in best_subset], lr=1e-2, epochs=2)\n",
    "final_gate.fit(pairs_tv, y_tv)\n",
    "elapsed_tv = time.time() - start_tv\n",
    "print(f\"   * Final gate retrained on Train+Valid in {elapsed_tv:.1f}s.\")\n",
    "\n",
    "# Save final gate state & selected indices\n",
    "CKPT = Path(\"models/gates/final_moe_gate.pt\")\n",
    "final_gate.gate.eval()\n",
    "torch.save(final_gate.gate.state_dict(), CKPT)\n",
    "\n",
    "IDX_FPATH = Path(\"models/gates/moe_selected_idxs.npy\")\n",
    "np.save(IDX_FPATH, np.array(best_subset))\n",
    "print(f\"   Saved gate state -> {CKPT}\")\n",
    "print(f\"   Saved selected indices -> {IDX_FPATH}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
