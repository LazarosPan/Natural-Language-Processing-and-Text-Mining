{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a9e4bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 04_models.ipynb  ·  Mixture‐of‐Experts training & automated tuning\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "# CELL 1 ────────────────────────────────────────────────────────────────────\n",
    "%run setup.py\n",
    "import numpy as np, pandas as pd, random, torch\n",
    "from pathlib import Path\n",
    "from itertools import combinations\n",
    "\n",
    "from src.models import default_experts, SBertExpert, MoEClassifier, get_predictions\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "\n",
    "DATA = Path(\"../data/splits\")\n",
    "train_df = pd.read_csv(DATA/\"train.csv\").dropna()\n",
    "valid_df = pd.read_csv(DATA/\"valid.csv\").dropna()\n",
    "\n",
    "pairs_tr  = list(zip(train_df.question1, train_df.question2))\n",
    "y_tr      = train_df.is_duplicate.values.astype(int)\n",
    "pairs_val = list(zip(valid_df.question1, valid_df.question2))\n",
    "y_val     = valid_df.is_duplicate.values.astype(int)\n",
    "\n",
    "EMB_PATH = \"../data/processed/question_embeddings.npy\"\n",
    "LR_PATH  = \"models/sbert_lr.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a386e5dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SBERT LR already present.\n"
     ]
    }
   ],
   "source": [
    "# ─── CELL 2 · Instantiate Experts & Fit SBERT’s LogisticRegression ──────────\n",
    "experts = default_experts(emb_path=EMB_PATH, lr_path=LR_PATH, embed_lr_ready=False)\n",
    "sbert   = next(e for e in experts if isinstance(e, SBertExpert))\n",
    "\n",
    "if not sbert.lr_path.exists():\n",
    "    meta = pd.read_csv(\"../data/processed/question_meta.csv\")\n",
    "    rev  = {q: i for i, q in enumerate(meta.question)}\n",
    "    q1   = train_df.question1.map(rev).values.astype(int)\n",
    "    q2   = train_df.question2.map(rev).values.astype(int)\n",
    "    sbert.fit(q1, q2, y_tr)\n",
    "    print(\"SBERT LR trained.\")\n",
    "else:\n",
    "    print(\"SBERT LR already present.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "437514f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expert forward-passes cached: (323554, 4)\n"
     ]
    }
   ],
   "source": [
    "# CELL 3 ─── pre-compute & cache expert outputs once ───────────────────────\n",
    "from sklearn.metrics import log_loss\n",
    "P_tr  = get_predictions(experts, pairs_tr,  \"train\")\n",
    "P_val = get_predictions(experts, pairs_val, \"valid\")\n",
    "print(\"Expert forward-passes cached:\", P_tr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5437f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded experts: {'BertExpert': 0, 'RobertaExpert': 1, 'SBertExpert': 2, 'CrossEncExpert': 3}\n",
      "Evaluating 15 distinct subsets…\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      ">> subset (0,)  (loaded)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "   valid log-loss = 0.1025\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      ">> subset (1,)  (loaded)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "   valid log-loss = 0.0571\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      ">> subset (2,)  (loaded)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "   valid log-loss = 0.4445\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      ">> subset (3,)  (loaded)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "   valid log-loss = 0.2393\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 1/2  ·  loss 0.0582\n",
      "Epoch 2/2  ·  loss 0.0257\n",
      "\n",
      ">> subset (0, 1)  (trained & cached)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "   valid log-loss = 0.0380\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 1/2  ·  loss 0.1320\n",
      "Epoch 2/2  ·  loss 0.1168\n",
      "\n",
      ">> subset (0, 2)  (trained & cached)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "   valid log-loss = 0.1041\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 1/2  ·  loss 0.1027\n",
      "Epoch 2/2  ·  loss 0.1276\n",
      "\n",
      ">> subset (0, 3)  (trained & cached)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "   valid log-loss = 0.1016\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 1/2  ·  loss 0.0264\n",
      "Epoch 2/2  ·  loss 0.0210\n",
      "\n",
      ">> subset (1, 2)  (trained & cached)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "   valid log-loss = 0.0424\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 1/2  ·  loss 0.0244\n",
      "Epoch 2/2  ·  loss 0.0185\n",
      "\n",
      ">> subset (1, 3)  (trained & cached)\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 44\u001b[39m\n\u001b[32m     41\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m>> subset \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midxs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m  (trained & cached)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     42\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[32m100\u001b[39m*\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m ll = \u001b[43mmoe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpairs_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   valid log-loss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mll\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ll < best_ll:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/1) Μεταπτυχιακό/2nd Semester/NLP/Natural-Language-Processing-and-Text-Mining/src/models.py:400\u001b[39m, in \u001b[36mMoEClassifier.evaluate\u001b[39m\u001b[34m(self, pairs, y)\u001b[39m\n\u001b[32m    396\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    397\u001b[39m \u001b[33;03mCompute binary log‐loss between labels y and predicted probabilities.\u001b[39;00m\n\u001b[32m    398\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m log_loss\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m p = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredict_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpairs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m log_loss(y, p)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.13/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/1) Μεταπτυχιακό/2nd Semester/NLP/Natural-Language-Processing-and-Text-Mining/src/models.py:389\u001b[39m, in \u001b[36mMoEClassifier.predict_prob\u001b[39m\u001b[34m(self, pairs)\u001b[39m\n\u001b[32m    384\u001b[39m \u001b[38;5;129m@torch\u001b[39m.no_grad()\n\u001b[32m    385\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpredict_prob\u001b[39m(\u001b[38;5;28mself\u001b[39m, pairs: List[Pair]) -> np.ndarray:\n\u001b[32m    386\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    387\u001b[39m \u001b[33;03m    For new pairs, return blended duplicate probabilities (shape=(n_pairs,)).\u001b[39;00m\n\u001b[32m    388\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m389\u001b[39m     probs = np.column_stack([\u001b[43mexp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpairs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m exp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.experts])\n\u001b[32m    390\u001b[39m     probs_t = torch.tensor(probs, dtype=torch.float32).to(_DEVICE)\n\u001b[32m    391\u001b[39m     weights = \u001b[38;5;28mself\u001b[39m.gate(probs_t)            \u001b[38;5;66;03m# (n_pairs, K)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/1) Μεταπτυχιακό/2nd Semester/NLP/Natural-Language-Processing-and-Text-Mining/src/models.py:86\u001b[39m, in \u001b[36mBaseExpert.predict_prob\u001b[39m\u001b[34m(self, pairs)\u001b[39m\n\u001b[32m     81\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     82\u001b[39m \u001b[33;03mInput : list[(q1, q2)] of raw strings\u001b[39;00m\n\u001b[32m     83\u001b[39m \u001b[33;03mOutput: np.ndarray of shape (len(pairs),), dtype=float32, values in [0,1]\u001b[39;00m\n\u001b[32m     84\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     85\u001b[39m \u001b[38;5;28mself\u001b[39m._ensure_loaded()\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predict_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpairs\u001b[49m\u001b[43m)\u001b[49m.astype(\u001b[33m\"\u001b[39m\u001b[33mfloat32\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.13/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/1) Μεταπτυχιακό/2nd Semester/NLP/Natural-Language-Processing-and-Text-Mining/src/models.py:157\u001b[39m, in \u001b[36m_HFAutoExpert._predict_impl\u001b[39m\u001b[34m(self, pairs)\u001b[39m\n\u001b[32m    155\u001b[39m     \u001b[38;5;66;03m# P(class=1) = duplicate probability\u001b[39;00m\n\u001b[32m    156\u001b[39m     p = torch.softmax(logits, dim=\u001b[32m1\u001b[39m)[:, \u001b[32m1\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m     probs.extend(\u001b[43mp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.tolist())\n\u001b[32m    158\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m np.array(probs, dtype=\u001b[33m\"\u001b[39m\u001b[33mfloat32\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# CELL 4 – Gate tuning over valid split  +  disk-cache for every subset\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import log_loss\n",
    "from src.models import save_gate, load_gate, _subset_key\n",
    "\n",
    "GATE_DIR = Path(\"models/gates\");  GATE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 1 · map class-name → index in `experts`\n",
    "idx_of = {e.__class__.__name__: i for i, e in enumerate(experts)}\n",
    "print(\"Loaded experts:\", idx_of)\n",
    "\n",
    "# 2 · build *all* non-empty subsets (1-to-full), drop duplicates automatically\n",
    "from itertools import combinations\n",
    "\n",
    "valid_subsets: list[tuple[int, ...]] = []\n",
    "for k in range(1, len(idx_of) + 1):                      # 1 … K experts\n",
    "    for tpl in combinations(idx_of.values(), k):\n",
    "        valid_subsets.append(tpl)\n",
    "\n",
    "print(f\"Evaluating {len(valid_subsets)} distinct subsets…\")\n",
    "\n",
    "best_ll, best_subset = 1e9, None\n",
    "\n",
    "for idxs in valid_subsets:\n",
    "    subset_exps = [experts[i] for i in idxs]\n",
    "    key         = _subset_key(subset_exps)\n",
    "    ckpt_path   = GATE_DIR / f\"gate_{key}.pt\"\n",
    "\n",
    "    # ── 2-a • load if already trained ──────────────────────────────────────\n",
    "    if ckpt_path.exists():\n",
    "        print(100*\"-\")\n",
    "        moe = load_gate(subset_exps, ckpt_path)\n",
    "        print(f\"\\n>> subset {idxs}  (loaded)\")\n",
    "        print(100*\"-\")\n",
    "    else:\n",
    "        # ── 2-b • train gate (only!) & save ────────────────────────────────\n",
    "        print(100*\"-\")\n",
    "        moe = MoEClassifier(subset_exps, lr=1e-2, epochs=2)\n",
    "        moe.fit(pairs_tr, y_tr)\n",
    "        save_gate(moe, ckpt_path)\n",
    "        print(f\"\\n>> subset {idxs}  (trained & cached)\")\n",
    "        print(100*\"-\")\n",
    "\n",
    "    ll = moe.evaluate(pairs_val, y_val)\n",
    "    print(f\"   valid log-loss = {ll:.4f}\")\n",
    "\n",
    "    if ll < best_ll:\n",
    "        best_ll, best_subset = ll, idxs\n",
    "        best_moe             = moe          # keep the loaded / trained model\n",
    "\n",
    "print(f\"\\nBEST subset {best_subset} · valid LL = {best_ll:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3542949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 5 ─── re-train BEST gate on Train+Valid & save ───────────────────────\n",
    "pairs_tv = pairs_tr + pairs_val\n",
    "y_tv     = np.concatenate([y_tr, y_val])\n",
    "\n",
    "final_gate = MoEClassifier([experts[i] for i in best_subset], lr=1e-2, epochs=2)\n",
    "final_gate.fit(pairs_tv, y_tv)\n",
    "\n",
    "CKPT = Path(\"models/moe_gate_state.pt\")\n",
    "torch.save(final_gate.gate.state_dict(), CKPT)\n",
    "np.save(\"models/moe_selected_idxs.npy\", np.array(best_subset))\n",
    "print(\"Saved:\", CKPT, \"and the subset indices.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
