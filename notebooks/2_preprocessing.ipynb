{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7334c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T20:22:40.042019Z",
     "iopub.status.busy": "2025-06-02T20:22:40.041932Z",
     "iopub.status.idle": "2025-06-02T20:23:33.290867Z",
     "shell.execute_reply": "2025-06-02T20:23:33.290654Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique questions across all splits: 537,359\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32ec6585f08b4c3188a874be7009ee88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1050 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768-dim SBERT embedding matrix shape: (537359, 768)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c206a262aa284fe682509c2f3beb4636",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1050 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384-dim SBERT embedding matrix shape: (537359, 384)\n",
      "\n",
      "Pre-processing complete. Files now in data/processed/:\n",
      " * clean_questions.npy\n",
      " * question_embeddings_384.npy\n",
      " * question_embeddings_768.npy\n",
      " * question_meta.csv\n",
      " * st_d2389aac.npy\n",
      " * st_d46f7701.npy\n"
     ]
    }
   ],
   "source": [
    "# ────────────────────────────────────────────────────────────────\n",
    "# notebooks/2_preprocessing.ipynb\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "# 0) Enable src/ on PYTHONPATH\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "%run setup.py\n",
    "\n",
    "# 1) Imports\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from src.preprocessing import clean_and_stats, build_st_embeddings\n",
    "\n",
    "# 2) Read split CSVs\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "SPLIT_DIR = Path(\"../data/splits\")\n",
    "train_df  = pd.read_csv(SPLIT_DIR / \"train.csv\").dropna(subset=[\"question1\", \"question2\"])\n",
    "valid_df  = pd.read_csv(SPLIT_DIR / \"valid.csv\").dropna(subset=[\"question1\", \"question2\"])\n",
    "test_df   = pd.read_csv(SPLIT_DIR / \"test.csv\").dropna(subset=[\"question1\", \"question2\"])\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# UNION ALL QUESTIONS\n",
    "#\n",
    "# We take the union of every unique string in question1/2 across\n",
    "# train/valid/test. Since our SBERT models are pretrained and\n",
    "# frozen, this does NOT leak label information.\n",
    "# -----------------------------------------------------------------\n",
    "all_questions = (\n",
    "    pd.concat(\n",
    "        [\n",
    "            train_df[[\"question1\", \"question2\"]],\n",
    "            valid_df[[\"question1\", \"question2\"]],\n",
    "            test_df[[\"question1\", \"question2\"]],\n",
    "        ],\n",
    "        axis=0,\n",
    "    )\n",
    "    .stack()\n",
    "    .unique()\n",
    ")\n",
    "\n",
    "print(f\"Unique questions across all splits: {len(all_questions):,}\")\n",
    "\n",
    "# 3) Clean texts & record char/word stats\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "cleaned, char_len, word_cnt = [], [], []\n",
    "for q in all_questions:\n",
    "    c, ln, wc = clean_and_stats(q)\n",
    "    cleaned.append(c)\n",
    "    char_len.append(ln)\n",
    "    word_cnt.append(wc)\n",
    "\n",
    "# 4) Persist per-question artefacts\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "OUT_DIR = Path(\"../data/processed\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        \"question\": all_questions,\n",
    "        \"clean\": cleaned,\n",
    "        \"len\": char_len,\n",
    "        \"words\": word_cnt,\n",
    "    }\n",
    ").to_csv(OUT_DIR / \"question_meta.csv\", index=False)\n",
    "\n",
    "np.save(OUT_DIR / \"clean_questions.npy\", np.array(cleaned, dtype=object))\n",
    "\n",
    "# 5) SBERT embeddings (768-dim & 384-dim), on-disk cache + logging\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# Remove stale files if present\n",
    "emb_768_fp = OUT_DIR / \"question_embeddings_768.npy\"\n",
    "if emb_768_fp.exists():\n",
    "    emb_768_fp.unlink()\n",
    "    print(\"Deleted stale question_embeddings_768.npy\")\n",
    "\n",
    "emb_384_fp = OUT_DIR / \"question_embeddings_384.npy\"\n",
    "if emb_384_fp.exists():\n",
    "    emb_384_fp.unlink()\n",
    "    print(\"Deleted stale question_embeddings_384.npy\")\n",
    "\n",
    "\n",
    "# 5a) Build / load 768-dim embeddings\n",
    "emb_768 = build_st_embeddings(\n",
    "    texts       = cleaned,\n",
    "    target_dim  = 768,\n",
    "    cache_dir   = OUT_DIR,            # hashed SBERT cache lives here\n",
    "    batch_size  = 512,\n",
    "    save_path   = emb_768_fp          # canonical downstream file\n",
    ")\n",
    "print(\"768-dim SBERT embedding matrix shape:\", emb_768.shape)\n",
    "\n",
    "\n",
    "# 5b) Build / load 384-dim embeddings\n",
    "emb_384 = build_st_embeddings(\n",
    "    texts       = cleaned,\n",
    "    target_dim  = 384,\n",
    "    cache_dir   = OUT_DIR,\n",
    "    batch_size  = 512,\n",
    "    save_path   = emb_384_fp\n",
    ")\n",
    "print(\"384-dim SBERT embedding matrix shape:\", emb_384.shape)\n",
    "\n",
    "\n",
    "print(\"\\nPre-processing complete. Files now in data/processed/:\")\n",
    "for p in sorted(glob.glob(str(OUT_DIR / '*'))):\n",
    "    print(\" *\", Path(p).name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
